---
title: "analysing"
output: html_document
---

0、词典问题：jieba字典组合为一个大txt
1、用并行方法实现每行分词：lapply改为parLapply、parLapplyLB、future_lapply等
2、当每个cl的任务中的函数需要第三方库，如何提前在每个cl中导入库：clusterEvalQ, clusterExport
4、主题模型
5、创建n元词组
6、group_by柱状图排序问题
7、停词管理


```{r}
#抓取函数需要的库
library(lubridate)
library(tidyverse)
library(rvest)
library(foreach)
library(doParallel)
library(parallel)

#分析需要的库
library(readxl) #read_xlsx
library(microbenchmark)
library(snow)   #snow.time
library(pryr)
library(tidytext)
library(jiebaR)
library(Rwordseg) #要装jdk，rjava并设置系统环境 http://jianl.org/cn/R/Rwordseg.html
library(tmcn)     #NTUSD: National Taiwan University Semantic Dictionary 中文情感极性词典;stopwordsCN() 中文停用词
library(devtools) #install_github()
library(cidian)   #可以安装搜狗词典,install_github("qinwf/cidian") 
library(topicmodels)  #LDA
source("xwlb_scraping_fun.r", encoding = "utf-8") #调用预设函数会调用脚本中的库
```

```{r 字典管理}
# jieba的字典：
# 1、可以自行设置txt文件放入工作环境中，建议使用notepad++编辑，将编码设置为utf-8，另存为txt文件
# 2、利用cidian库将搜狗词库scel转化为dict文件放到jieba的Dict文件夹中
show_dictpath()       #词典存放路径
dir(show_dictpath())  #该路径下的词典文件

#词典格式转化,将字典替换jiebaRD路径下的用户默认词典user.dict.utf8
decode_scel(scel = "./Dict/党建理论词汇.scel", cpp = TRUE)
scan(file = "./Dict/党建理论词汇.scel_2020-02-12_22_52_14.dict", what=character(), nlines=30, 
     sep='\n', encoding='utf-8', fileEncoding='utf-8')

#替换后的用户默认词典，或者直接将该文件转换为txt，用在worker的user参数中
scan(file = "D:/Program Files/Microsoft/R Open/R-3.5.3/library/jiebaRD/dict/user.dict.utf8", 
     what=character(), nlines=30, sep='\n', encoding='utf-8', fileEncoding='utf-8')

#输出tmcn包的中文停词，用在worker中
#write_delim(tibble(stopwordsCN()),"stopwordsCN.txt",col_names = FALSE) 

# dict <- dir("./Dict") #查看文件夹文件,等于list.file
# for (i in 1:length(dict)){
#   installDict(paste("./Dict", dict[i], sep='/'), dictname = dict[i]) #Rwordseg的installDict函数
# }   #循环安装字典
# listDict()  #需要cidian库的decode_scel函数将scel文件转为dict文件
```

```{r 抓取网页}
news_result <- all_scraping_fun_new()  #自动侦测末页页码
head(news_result)
object_size(news_result)
class(news_result)
stopCluster(cl)     #在函数中停止没有用
getDoParWorkers()   #看有多少worder
getDoParName()      #看集群名称
```

```{r 内容初步清理}
news_result %>% 
  mutate(content = str_replace_all(content,
        c("各位观众晚上好今天是年月日欢迎您收看今天的新闻联播节目今天节目的主要内容有" = "",
          "央视网消息新闻联播完整版" = "",
          "央视网消息新闻联播文字版" = "",
          "新闻特写" = "",
          "央视快评" = "",
          "新春走基层" = "",
          "国内联播快讯" = "",
          "国际联播快讯" = "",
          "新闻联播" = "",
          "央视网" = ""))) -> news_result
head(news_result)
```

```{r 分词函数测试}
# #取10条做测试
# content_test <- head(news_result, 10)  
# content_test
# 
# #jieba分词：全部文本统一一拆分
# seg_jb <- segment(content_test$content, worker())
# seg_jb
# 
# #Rwordseg分词(向量)：每行为一个列表
# seg_ansj_v <- segmentCN(content_test$content, returnType = "vector")
# seg_ansj_v
# 
# #Rwordseg分词(tm)：每行一个向量
# seg_ansj_tm <- segmentCN(content_test$content, returnType = "tm")
# seg_ansj_tm

#get_tuple(segment(content_test$content, worker()), size = 2)
```


```{r 分词自定义函数测试}
# #jieba：每行返回一个列表，每个列表是分开的词
# seg_jb_la <- lapply(content_test$content, function(x) {
#   tokens <- segment(x, jiebar = jieba_worker)
#   tokens <- tokens[nchar(tokens) > 1]
#   return(tokens)
#   })
# seg_jb_la
# 
# #并行函数分解测试
# cl <- makeCluster(detectCores(logical = FALSE))
# clusterEvalQ(cl, {
#   library(jiebaR)   #在每个worker中载入库
#   jieba_worker <- worker(stop_word = "stopwordsCN.txt") #设置分词引擎
#   }) 
# seg_jb_la_par <- parLapply(cl, content_test$content, function(x) {
#   tokens <- segment(x, jiebar = jieba_worker)
#   tokens <- tokens[nchar(tokens) > 1]
#   return(tokens)
# })
# stopCluster(cl)
# seg_jb_la_par
# 
# #Rwordseg分词(向量)：每行返回一个列表，每个列表是分开的词
# seg_ansj_v_la <- lapply(content_test$content, function(x) {
#   tokens <- segmentCN(x, returnType = "vector")
#   tokens <- tokens[nchar(tokens) > 1]
#   return(tokens)
#   })
# seg_ansj_v_la
# 
# #Rwordseg分词(tm)：每行返回一个列表，每个列表是一个完整向量，没有筛选词长度
# seg_ansj_tm_la <- lapply(content_test$content, function(x) {
#   tokens <- segmentCN(x, returnType = "tm")
#   tokens <- tokens[nchar(tokens) > 1]
#   return(tokens)
#   })
# seg_ansj_tm_la
```


```{r 设定分词函数}
#jieba函数
jieba_tokenizer <- function(t) {
  jieba_worker <- worker(stop_word = "stopwordsCN.txt")  #设定分词引擎
  lapply(t, function(x) {
    tokens <- segment(x, jiebar = jieba_worker)
    tokens <- tokens[nchar(tokens) > 1] #筛选长度>1的词
    return(tokens)
  })
}

#jieba并行函数
jieba_tokenizer_par <- function(t) {
  parLapply(cl, t, function(x) {
  tokens <- segment(x, jiebar = jieba_worker)
  tokens <- tokens[nchar(tokens) > 1]
  return(tokens)
  })
}

#jieba并行函数:load balancing
jieba_tokenizer_par_lb <- function(t) {
  parLapplyLB(cl, t, function(x) {
  tokens <- segment(x, jiebar = jieba_worker)
  tokens <- tokens[nchar(tokens) > 1]
  return(tokens)
  })
}

#jieba生成二元词组函数
jieba_tokenizer_ngram <- function(t) {
  jieba_worker <- worker(stop_word = "stopwordsCN.txt")  #设定分词引擎
  lapply(t, function(x) {
    ngram <- get_tuple(segment(x, jiebar = jieba_worker), size = 2)
    return(ngram)
  })
}


# #ansj函数：输出向量
# ansj_tokenizer <- function(t) {                      
#   lapply(t, function(x) {
#     tokens <- segmentCN(x, returnType = "vector")
#     tokens <- tokens[nchar(tokens) > 1] #筛选长度>1的词
#     return(tokens)
#   })
# }
# 
# #ansj函数2：输出tm
# ansj_tokenizer <- function(t) {                      
#   lapply(t, function(x) {
#     tokens <- segmentCN(x, returnType = "tm")
#     tokens <- tokens[nchar(tokens) > 1] #筛选长度>1的词
#     return(tokens)
#   })
# }
```


```{r 中文分词}
#使用jieba方案，比Rwordseg方案快很多，效果最理想
news_result %>% unnest_tokens(input = content, output = word, token = jieba_tokenizer) -> news_result_tidy_jb
news_result_tidy_jb
```


```{r 多线程测试}
#设置线程数
cl <- makeCluster(detectCores(logical = FALSE))
#在每个worker中载入相同的库和分词引擎
clusterEvalQ(cl, {
  library(jiebaR)
  jieba_worker <- worker(stop_word = "stopwordsCN.txt")
  })
#运行分词处理
news_result %>% unnest_tokens(input = content, output = word, token = jieba_tokenizer_par) -> news_result_tidy_jb_par
#运算时间可视化
st_par <- snow.time(news_result %>% unnest_tokens(input = content, output = word, token = jieba_tokenizer_par))
plot(st_par)
st_par_lb <- snow.time(news_result %>% unnest_tokens(input = content, output = word, token = jieba_tokenizer_par_lb))
plot(st_par_lb)
#停止多线程
stopCluster(cl)
news_result_tidy_jb_par
```


```{r 速度测试}
# cl <- makeCluster(detectCores(logical = FALSE))
# clusterEvalQ(cl, {
#   library(jiebaR)
#   jieba_worker <- worker(stop_word = "stopwordsCN.txt")
#   }) 
# microbenchmark(
#   seq = news_result %>% unnest_tokens(input = content, output = word, token = jieba_tokenizer),
#   par = news_result %>% unnest_tokens(input = content, output = word, token = jieba_tokenizer_par),
#   par_lb = news_result %>% unnest_tokens(input = content, output = word, token = jieba_tokenizer_par_lb),
#   times = 10,
#   unit = "s"
# )
# stopCluster(cl)
```
在6核的条件下，并行方案是非并行方案速度的2-3倍，而速度最快的是一般并行方案，随着核数增多，LB方案消耗的时间会增加，反而不如一般并行方案。

```{r 进一步处理文本数据, fig.width=20, fig.height=10}
news_result_tidy_jb %>% 
  group_by(date) %>% 
  mutate(id = row_number(),
         word_sum_1_3 = round(n() / 3)) %>% 
  filter(id <= word_sum_1_3 & date >= "2019-01-01") %>% 
  summarise(xjp = sum(word=="习近平"),
            word_sum = n(),
            prop = xjp / word_sum) %>% 
  ggplot(aes(date, prop)) +
  geom_line() +
  geom_point() +
  geom_smooth(method = 'loess')
```

```{r 导入情绪词典}
#台湾大学简体中文情感极性词典ntusd
ntusd_positive <- read_csv("./台湾大学的极性词/NTUSD_positive_simplified.txt", col_names = "positive")
ntusd_negative <- read_csv("./台湾大学的极性词/NTUSD_negative_simplified.txt", col_names = "negative")
ntusd_positive
ntusd_negative

#清华大学李军中文褒贬义词典
qh_praise <- read_csv("./清华大学李军中文褒贬义词典/praise.txt", col_names = "praise")
qh_degrade <- read_csv("./清华大学李军中文褒贬义词典/degrade.txt", col_names = "degrade")
qh_praise
qh_degrade

#大连理工大学情感词汇本体库
dl_benti <- read_xlsx("./大连理工大学情感词汇本体库/情感词汇本体.xlsx")
dl_benti
```

```{r 匹配情感词典, fig.width=20, fig.height=10}
news_result_tidy_jb %>% 
  mutate(positive = if_else(word %in% ntusd_positive$positive, 1, 0),
         negative = if_else(word %in% ntusd_negative$negative, 1, 0),
         praise = if_else(word %in% qh_praise$praise, 1, 0),
         degrade = if_else(word %in% qh_degrade$degrade, 1, 0),
         good = if_else(word %in% ntusd_positive$positive | word %in% qh_praise$praise, 1, 0),
         bad = if_else(word %in% ntusd_negative$negative | word %in% qh_degrade$degrade, 1, 0)) %>% 
  group_by(date) %>% 
  summarise(positive_sum = sum(positive),
            negative_sum = sum(negative),
            praise_sum = sum(praise),
            degrade_sum = sum(degrade),
            good_sum = sum(good),
            bad_sum = sum(bad)) %>% 
  mutate(positive_score = (positive_sum - negative_sum) / (positive_sum + negative_sum),
         praise_score = (praise_sum - degrade_sum) / (praise_sum + degrade_sum),
         good_score = (good_sum - bad_sum) / (good_sum + bad_sum)) %>% 
  select(date, positive_score:good_score) %>% 
  gather(key = score_type, value = score, positive_score:good_score) %>% 
  arrange(date) %>% 
  ggplot(aes(date, score, color = score_type), ) +
  geom_line() +
  geom_smooth(method = "loess") + 
  theme(axis.title = element_text(size = 25),
        axis.text = element_text(size = 15),
        legend.text = element_text(size=15),
        legend.title = element_text(size = 20),
        legend.position = "right")
```

```{r 最常见的正面/负面词}
news_result_tidy_jb %>% 
  mutate(positive = if_else(word %in% ntusd_positive$positive, 1, 0),
         negative = if_else(word %in% ntusd_negative$negative, 1, 0),
         praise = if_else(word %in% qh_praise$praise, 1, 0),
         degrade = if_else(word %in% qh_degrade$degrade, 1, 0),
         good = if_else(word %in% ntusd_positive$positive | word %in% qh_praise$praise, 1, 0),
         bad = if_else(word %in% ntusd_negative$negative | word %in% qh_degrade$degrade, 1, 0)) -> news_result_score

news_result_score %>% filter(positive == 1) %>% count(word) %>% top_n(wt = n, n = 10) %>% arrange(desc(n))
news_result_score %>% filter(negative == 1) %>% count(word) %>% top_n(wt = n, n = 10) %>% arrange(desc(n))
news_result_score %>% filter(praise == 1) %>% count(word) %>% top_n(wt = n, n = 10) %>% arrange(desc(n))
news_result_score %>% filter(degrade == 1) %>% count(word) %>% top_n(wt = n, n = 10) %>% arrange(desc(n))
```

```{r 主题模型}
#需要先将tidy格式转为dtm格式
news_result_tidy_jb %>% 
  count(date, word) %>% 
  cast_dtm(date, word, n) -> news_result_dtm #转化速度很快
news_result_dtm
object_size(news_result_dtm)
```

```{r 调用LDA算法}
#LDA算法
news_lda_5 <- LDA(news_result_dtm, k = 5, control = list(seed = 100)) #计算时间较长
news_lda_5
class(news_lda_5)
str(news_lda_5)
terms(news_lda_5, 5) #每个主题概率(beta)最高5个词
```

```{r 输出每个词属于每个主题的概率, fig.height=8, fig.width=20}
#重新转为tidy格式
news_lda_5_tidy_word <- tidy(news_lda_5, matrix = "beta")
news_lda_5_tidy_word
news_lda_5_tidy_word %>% group_by(topic) %>% add_tally(beta) #每个主题所有词概率总和=1
#每个主题最具代表性的10个词
# news_lda_5_tidy_word %>% 
#   group_by(topic) %>% 
#   top_n(wt = beta, n = 10) %>% 
#   ggplot(aes(reorder(term,beta), beta, fill = as.factor(topic))) +
#   geom_col() +
#   coord_flip() +
#   facet_wrap(~topic, nrow = 1, scales = "free") +
#   theme(axis.text = element_text(size=15))

#确保每个group中降序排列的写法
#https://trinkerrstuff.wordpress.com/2016/12/23/ordering-categories-within-ggplot2-facets/
news_lda_5_tidy_word %>% 
  group_by(topic) %>% 
  top_n(wt = beta, n = 10) %>% 
  arrange(desc(beta)) %>% 
  ungroup() %>% 
  mutate(term = factor(paste(term, topic, sep = "_"), levels = rev(paste(term, topic, sep = "_")))) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +  
  facet_wrap(~topic, nrow = 1, scales = "free") +  
  theme(axis.text = element_text(size=15),
        axis.title = element_text(size = 15),
        strip.text = element_text(size = 20)) +
  scale_x_discrete(labels = function(x) gsub("_.+$", "", x))
```

```{r 输出每个文档中的词属于每个主题的概率, fig.height=8, fig.width=20}
#重新转为tidy格式
news_lda_5_tidy_doc <- tidy(news_lda_5, matrix = "gamma")
news_lda_5_tidy_doc
news_lda_5_tidy_doc %>% group_by(document) %>% add_tally(gamma) #每个文档gamma之和=1
```

```{r 将每个文档每个单词分配给一个主题}
 #将每个词归属给概率最高的那个主题
assign <- augment(news_lda_5, data = news_result_dtm) 
assign
```


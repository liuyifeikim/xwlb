---
title: "Untitled"
output: word_document
---

1、如何建立存储效率较高的矩阵格式
2、如何建立相似度矩阵
3、如何建立简单的基于内容的推荐系统，以相似度为主要计算依据

```{r, message=FALSE}
library(tidyverse)
library(tidytext)
library(jiebaR)
library(parallel)
library(pryr)
library(FNN)
library(Matrix)
library(furrr)
library(tictoc)
```


```{r}
news_result <- read_csv("news_result(20200907).csv", locale = locale(encoding = "GBK"))
```


```{r}
news_result <- news_result %>% 
  distinct(date, .keep_all = TRUE) %>% 
  mutate(len = str_length(content)) %>% 
  filter(len > 100) %>% 
  select(-len) %>% 
  arrange(date)
news_result %>% head()
news_result %>% dim()
```

```{r}
# 分词函数
jieba_tokenizer <- function(t) {
  jieba_worker <- worker(stop_word = "stopwordsCN.txt")  #设定分词引擎
  lapply(t, function(x) {
    tokens <- segment(x, jiebar = jieba_worker)
    tokens <- tokens[nchar(tokens) > 1] #筛选长度>1的词
    return(tokens)
  })
}
jieba_tokenizer_par <- function(t) {
  parLapply(cl, t, function(x) {
    tokens <- segment(x, jiebar = jieba_worker)
    tokens <- tokens[nchar(tokens) > 1]
    return(tokens)
  })
  }
```


```{r}
# 分词
cl <- makeCluster(detectCores())
clusterEvalQ(
  cl = cl, 
  expr = {
    library(jiebaR)
    jieba_worker <- worker(stop_word = "stopwordsCN.txt")
  })
news_result_tidy_jb <- news_result %>% unnest_tokens(input = content, output = word, token = jieba_tokenizer_par)
stopCluster(cl)
news_result_tidy_jb
```


```{r}
# 提取100个关键词
news_result_top_100 <- news_result_tidy_jb %>% 
  count(date, word) %>% 
  bind_tf_idf(document = date, term = word, n = n) %>% 
  group_by(date) %>% 
  slice_max(n = 100, order_by = tf_idf)
news_result_top_100
```


```{r}
# 转化为dtm，作为转换matrix的中介
news_result_top_100_dtm <- cast_dtm(data = news_result_top_100,
                                    document = date,
                                    term = word,
                                    value = tf_idf)
news_result_top_100_dtm
object_size(news_result_top_100_dtm)
```


```{r}
# 转化为矩阵
news_result_top_100_mat <- as.matrix(news_result_top_100_dtm)
object_size(news_result_top_100_mat)
dim(news_result_top_100_mat)
news_result_top_100_mat[1:10, 1:10]
```

```{r}
# 计算最相似的K个近邻
# knn_res <- get.knn(news_result_top_100_mat, k = 5) # 输入一般矩阵
# knn_res$nn.index %>% head(10)
```

```{r}
# 手动构建dtm
# news_result_top_100_wide <- news_result_top_100 %>% 
#   pivot_wider(id_cols = date, names_from = word, values_from = tf_idf, values_fill = 0) %>%
#   ungroup(date) %>% 
#   select(-date)
# news_result_top_100_wide %>% head()
# object_size(news_result_top_100_wide)
```

```{r}
# 计算最相似的K个近邻
# knn_res_wide <- get.knn(news_result_top_100_wide, k = 5) # 输入tibble
# knn_res_wide$nn.index %>% head(10)
```

```{r}
# 直接将宽数据转为稀疏矩阵
# row_names <- unique(news_result_top_100$date)
# news_result_top_100_wide_temp <- news_result_top_100 %>% 
#   pivot_wider(id_cols = date, names_from = word, values_from = tf_idf, values_fill = 0) %>%
#   ungroup(date) %>% 
#   select(-date)
# col_names <- colnames(news_result_top_100_wide_temp)
# news_result_top_100_wide_smat <- Matrix(data = news_result_top_100_wide_temp, 
#                                         nrow = nrow(news_result_top_100_wide_temp),
#                                         ncol = ncol(news_result_top_100_wide_temp),
#                                         dimnames = list(row_names, col_names),
#                                         sparse = TRUE)
```

```{r}
# news_result_top_100_dtm_smat <- Matrix(data = news_result_top_100_dtm, 
#                                        nrow = news_result_top_100_dtm$nrow,
#                                        ncol = news_result_top_100_dtm$ncol,
#                                        dimnames = list(news_result_top_100_dtm$dimnames$Docs, 
#                                                        news_result_top_100_dtm$dimnames$Terms),
#                                        sparse = TRUE)
```


```{r}
# 转为稀疏矩阵
col_names <- dimnames(news_result_top_100_mat)$Terms
row_names <- dimnames(news_result_top_100_mat)$Docs
news_result_top_100_smat <- Matrix(news_result_top_100_mat, dimnames = list(row_names, col_names), sparse = TRUE)
news_result_top_100_smat[1:10, 1:10]
object_size(news_result_top_100_smat)
news_result_top_100_smat@Dim
```

```{r}
# 计算最相似的K个近邻
knn_res_smat <- get.knn(news_result_top_100_smat, k = 5) # 输入稀疏矩阵
knn_res_smat$nn.index %>% head(10)
```


```{r}
knn_res_smat$nn.index[1,]
recommend_id <- knn_res_smat$nn.index[1, ]
recommend_id
news_result %>% slice(recommend_id)
```


```{r}
recommend_fun <- function(i){
  recommend_id <- knn_res_smat$nn.index[i, ]
  recommend_news <- news_result %>% slice(recommend_id) %>% select(date, content)
  return(recommend_news)
}
```



```{r}
plan(multisession)
recommend_result <- news_result %>%
  mutate(id = row_number(),
         recommend = future_map(id, recommend_fun)) %>% 
  unnest(recommend, names_repair = "universal") %>% 
  select(-id, -content...2) %>% 
  rename(date_origin = date...1,
         date_recommend = date...4,
         content_recommend = content...5)
plan(sequential)

recommend_result %>% head()
```


```{r}
total_recommend_fun <- function(tibble, keyword_num  = 100, k = 5){
  
  # 载入库
  library(tidyverse)
  library(tidytext)
  library(jiebaR)
  library(parallel)
  library(furrr)
  library(FNN)
  library(Matrix)

  # 预处理
  tibble <- tibble %>% 
    distinct(date, .keep_all = TRUE) %>% 
    mutate(len = str_length(content)) %>% 
    filter(len > 100) %>% 
    select(-len) %>% 
    arrange(date)
  
  # 分词
  cl <- makeCluster(detectCores())
  clusterEvalQ(
    cl = cl, 
    expr = {
      library(jiebaR)
      jieba_worker <- worker(stop_word = "stopwordsCN.txt")
      })
  jieba_tokenizer_par <- function(t) {
    parLapply(cl, t, function(x) {
      tokens <- segment(x, jiebar = jieba_worker)
      tokens <- tokens[nchar(tokens) > 1]
      return(tokens)
    })
    }
  tibble_tidy_jb <- tibble %>% unnest_tokens(input = content, output = word, token = jieba_tokenizer_par)
  stopCluster(cl)
  
  # 提取关键词
  tibble_top <- tibble_tidy_jb %>% 
    count(date, word) %>% 
    bind_tf_idf(document = date, term = word, n = n) %>% 
    group_by(date) %>% 
    slice_max(n = keyword_num, order_by = tf_idf)
  
  # 转为稀疏矩阵
  tibble_top_dtm <- cast_dtm(data = tibble_top, document = date, term = word, value = tf_idf)
  tibble_top_mat <- as.matrix(tibble_top_dtm)
  col_names <- dimnames(tibble_top_mat)$Terms
  row_names <- dimnames(tibble_top_mat)$Docs
  tibble_top_smat <- Matrix(tibble_top_mat, dimnames = list(row_names, col_names), sparse = TRUE)
  
  # 相似度最高的k条记录
  knn_res <- get.knn(tibble_top_smat, k = k)
  
  # 输出推荐结果
  recommend_fun <- function(i){
    recommend_id <- knn_res$nn.index[i, ]
    recommend_news <- tibble %>% slice(recommend_id) %>% select(date, content)
    return(recommend_news)
    }
  plan(multisession)
  result <- tibble %>%
    mutate(id = row_number(),
           recommend = future_map(id, recommend_fun)) %>% 
    unnest(recommend, names_repair = "universal") %>% 
    select(-id, -content...2) %>% 
    rename(date_origin = date...1,
           date_recommend = date...4,
           content_recommend = content...5)
  plan(sequential)
  
  # 返回结果
  return(result)
  
  }
```


```{r}
tic()
news_recommend_result <- total_recommend_fun(news_result)
toc()
news_recommend_result
```



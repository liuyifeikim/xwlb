---
title: "Untitled"
output: word_document
---

1、如何建立存储效率较高的矩阵格式
2、如何建立相似度矩阵
3、如何建立简单的基于内容的推荐系统，以相似度为主要计算依据

```{r, message=FALSE}
library(tidyverse)
library(tidytext)
library(jiebaR)
library(parallel)
library(furrr)
library(pryr)
library(FNN)
library(Matrix)
library(foreach)
library(furrr)
```


```{r}
news_result <- read_csv("news_result(20200907).csv", locale = locale(encoding = "GBK"))
news_result %>% head()
news_result <- news_result %>% 
  filter(!is.na(content))
news_result %>% head()
```

```{r}
# 分词函数
jieba_tokenizer <- function(t) {
  jieba_worker <- worker(stop_word = "stopwordsCN.txt")  #设定分词引擎
  lapply(t, function(x) {
    tokens <- segment(x, jiebar = jieba_worker)
    tokens <- tokens[nchar(tokens) > 1] #筛选长度>1的词
    return(tokens)
  })
}
jieba_tokenizer_par <- function(t) {
  parLapply(cl, t, function(x) {
    tokens <- segment(x, jiebar = jieba_worker)
    tokens <- tokens[nchar(tokens) > 1]
    return(tokens)
  })
  }
```


```{r}
# 分词
cl <- makeCluster(detectCores())
clusterEvalQ(
  cl = cl, 
  expr = {
    library(jiebaR)
    jieba_worker <- worker(stop_word = "stopwordsCN.txt")
  })
news_result_tidy_jb <- news_result %>% unnest_tokens(input = content, output = word, token = jieba_tokenizer_par)
stopCluster(cl)
news_result_tidy_jb
```


```{r}
# 提取100个关键词
news_result_top_100 <- news_result_tidy_jb %>% 
  count(date, word) %>% 
  bind_tf_idf(document = date, term = word, n = n) %>% 
  group_by(date) %>% 
  slice_max(n = 100, order_by = tf_idf)
news_result_top_100
```

```{r}
# 转化为dtm，作为转换matrix的中介
news_result_top_100_dtm <- cast_dtm(data = news_result_top_100,
                                    document = date,
                                    term = word,
                                    value = tf_idf)
news_result_top_100_dtm
object_size(news_result_top_100_dtm)
```


```{r}
# 转化为矩阵
news_result_top_100_mat <- as.matrix(news_result_top_100_dtm)
object_size(news_result_top_100_mat)
dim(news_result_top_100_mat)
news_result_top_100_mat[1:10, 1:10]
```

```{r}
# 计算最相似的K个近邻
knn_res <- get.knn(news_result_top_100_mat, k = 5)
knn_res$nn.index %>% head(10)
```

```{r}
# 手动构建dtm
news_result_top_100_wide <- news_result_top_100 %>% 
  pivot_wider(id_cols = date, names_from = word, values_from = tf_idf, values_fill = 0) %>%
  ungroup(date) %>% 
  select(-date)
news_result_top_100_wide %>% head()
object_size(news_result_top_100_wide)
```

```{r}
# 计算最相似的K个近邻
knn_res_wide <- get.knn(news_result_top_100_wide, k = 5)
knn_res_wide$nn.index %>% head(10)
```


```{r}
# 转为稀疏矩阵
col_names <- dimnames(news_result_top_100_mat)$Terms
row_names <- dimnames(news_result_top_100_mat)$Docs
news_result_top_100_smat <- Matrix(news_result_top_100_mat, dimnames = list(row_names, col_names), sparse = TRUE)
news_result_top_100_smat[1:10, 1:10]
object_size(news_result_top_100_smat)
news_result_top_100_smat@Dim
```

```{r}
# 计算最相似的K个近邻
knn_res_smat <- get.knn(news_result_top_100_smat, k = 5)
knn_res_smat$nn.index %>% head(10)
```

```{r}
dim(knn_res_smat$nn.index)
```


```{r}
knn_res_smat$nn.index[1,]
recommend_id <- knn_res_smat$nn.index[1, ]
recommend_id
news_result[recommend_id, 2]
```


```{r}
recommend_fun <- function(i){
  recommend_id <- knn_res_smat$nn.index[i, ]
  recommend_news <- news_result[recommend_id, 2]
  return(recommend_news)
}
```


```{r}
plan(multisession)
news_result %>%
  mutate(recommend = future_map(id, recommend_fun)) %>% 
  unnest(recommend) %>% 
  select(-content)
plan(sequential)
```


```{r}
total_recommend_fun <- function(tibble, keyword_num  = 100, k = 5){
  # 删除缺失值
  tibble <- tibble %>% filter(!is.na(content))
   # 分词
  cl <- makeCluster(detectCores())
  clusterEvalQ(
    cl = cl, 
    expr = {
      library(jiebaR)
      jieba_worker <- worker(stop_word = "stopwordsCN.txt")
      })
  jieba_tokenizer_par <- function(t) {
    parLapply(cl, t, function(x) {
      tokens <- segment(x, jiebar = jieba_worker)
      tokens <- tokens[nchar(tokens) > 1]
      return(tokens)
    })
    }
  tibble_tidy_jb <- tibble %>% unnest_tokens(input = content, output = word, token = jieba_tokenizer_par)
  
  # 提取关键词
  tibble_top <- tibble_tidy_jb %>% 
    count(date, word) %>% 
    bind_tf_idf(document = date, term = word, n = n) %>% 
    group_by(date) %>% 
    slice_max(n = keyword_num, order_by = tf_idf)
  
  # 转为稀疏矩阵
  row_names <- rownames(tibble_top)
  col_names <- colnames(tibble_top)
  tibble_top_smat <- Matrix(tibble_top, dimnames = list(row_names, col_names), sparse = TRUE)
  
  # 相似度最高的k条记录
  knn_res_smat <- get.knn(tibble_top_smat, k = k)
  
  # 输出推荐结果
  recommend_fun <- function(i){
    recommend_id <- knn_res_smat$nn.index[i, ]
    recommend_news <- news_result[recommend_id, 2]
    return(recommend_news)
    }
  plan(multisession)
  news_recommend_result <- news_result %>%
    mutate(recommend = future_map(date, recommend_fun)) %>% 
    unnest(recommend) %>% 
    select(-content)
  plan(sequential)
  
  # 返回结果
  return(news_recommend_result)
  
  }
```


```{r}
news_recommend_result <- total_recommend_fun(news_result)
news_recommend_result
```


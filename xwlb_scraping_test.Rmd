---
title: "新闻联播"
output: html_document
---

1、定时爬取
2、source调用函数，有中文会报错

```{r}
library(lubridate)
library(stringr)
library(tidyverse)
library(rvest)
library(foreach)
library(doParallel)
library(microbenchmark)
library(pryr)
library(parallel)
```


```{r 单页面测试：内文url}
#提取内文url
out_url <- "http://www.xwlb.net.cn/video_1.html"
page <- read_html(out_url)
page %>% html_nodes("h2 a") %>% html_attr("href")
```


```{r 单页面测试:内容抓取}
#提取页面
in_url <- "http://www.xwlb.net.cn/11199.html"
news <- read_html(in_url)
#提取日期
news %>% 
  html_nodes(".xwlb h2") %>% 
  html_text(trim = TRUE) %>% 
  str_replace_all(c("《新闻联播》主要内容" = "",  "[年|月|日]" = "")) %>% 
  ymd() -> date
#提取链接
news %>% 
  html_node(".content") %>% 
  html_text(trim = TRUE) %>% 
  str_replace_all(c("\\W" = "", "\\d" = "", "[a-zA-Z]" = "")) -> content
tibble(date, content)
```

```{r 字符拼接测试}
num <- 11199
str_glue("http://www.xwlb.net.cn/{num}.html")
seq(1:10)
```

```{r 内文url抓取函数}
in_url_scraping_fun <- function(i){                                 #i为页码
  out_url <- str_glue("http://www.xwlb.net.cn/video_{i}.html")      #列表页url
  page <- read_html(out_url)                                        #读取列表页内容
  page %>% html_nodes("h2 a") %>% html_attr("href") -> in_url       #提取每页的内页链接
  return(in_url)  #可以不需要return，无return的话套用函数后不显示结果，但赋值后结果一样
}
in_url_scraping_fun(1)
```


```{r 内文url抓取测试}
registerDoParallel(cores = detectCores()) #doParallel包
in_url_list <- unlist(foreach(i = seq(1, 68)) %dopar% in_url_scraping_fun(i)) #循环抓取所有列表页的内文链接
in_url_list[1:10]
length(in_url_list)
```

```{r 并行测试}
microbenchmark(
  do = foreach(i = seq(1, 68)) %do% in_url_scraping_fun(i),
  do_par = foreach(i = seq(1, 68)) %dopar% in_url_scraping_fun(i),    #7倍速度
  times = 3,
  unit = "s"
)
```


```{r 内容面抓取函数}
news_scraping_fun <- function(in_url){
  #读取网页
  news <- read_html(in_url)
  #提取日期
  news %>% 
  html_node(".xwlb h2") %>% 
  html_text(trim = TRUE) %>% 
  str_replace_all(c("《新闻联播》主要内容" = "",  "[年|月|日]" = "")) %>% 
  ymd() -> date
  #提取内容
  news %>% 
  html_node(".content") %>% 
  html_text(trim = TRUE) %>% 
  str_replace_all(c("\\W" = "", "\\d" = "", "[a-zA-Z]" = "")) -> content
  news_tib <- tibble(date, content)
  return(news_tib)
}
news_scraping_fun("http://www.xwlb.net.cn/11218.html")
```
```{r 自动提取末页页码}
out_url <- "http://www.xwlb.net.cn/video_1.html"
page <- read_html(out_url)
page %>% html_nodes("a:nth-child(13)") %>% html_attr("href") %>% str_extract("\\d+") %>% as.integer()
```



```{r 函数组合}
all_scraping_fun <- function(last_page){      #输入末页页码
  #调用多线程
  registerDoParallel(cores = detectCores())
  #循环所有列表页，提取内页url列表
  in_url_list <- unlist(foreach(i = seq(1, last_page)) %dopar% in_url_scraping_fun(i))
  #从从内页循环提取新闻内容，并按行堆叠
  foreach(j = seq(1, length(in_url_list)), .combine = rbind) %dopar% news_scraping_fun(in_url_list[j])
  #返回结果
  #return(result)  #不用也可以
}
news_result <- all_scraping_fun(68)
news_result
object_size(news_result)
```

```{r 默认输入末页}
all_scraping_fun_new <- function(){
  #末页页码
  out_url <- str_glue("http://www.xwlb.net.cn/video_1.html")        #列表页url
  page <- read_html(out_url)                                        #读取列表页内容
  last_page <- page %>% html_nodes("a:nth-child(13)") %>% html_attr("href") %>% str_extract("\\d+") %>% as.integer()
  #调用多线程:snow的sock集群
  cl <- makeCluster(cl)
  registerDoParallel(cl)
  #循环所有列表页，提取内页url列表
  in_url_list <- unlist(foreach(i = seq(1, last_page)) %dopar% in_url_scraping_fun(i))
  #从内页循环提取新闻内容，并按行堆叠
  foreach(j = seq(1, length(in_url_list)), .combine = rbind) %dopar% news_scraping_fun(in_url_list[j])
}
news_result <- all_scraping_fun_new()
news_result
object_size(news_result)
```


```{r}
#单线程函数
all_scraping_fun_do <- function(last_page){      #输入末页页码
  #循环所有列表页，提取内页url列表
  in_url_list <- unlist(foreach(i = seq(1, last_page)) %do% in_url_scraping_fun(i))
  #从从内页循环提取新闻内容，并按行堆叠
  result <- foreach(j = seq(1, length(in_url_list)),.combine = rbind) %do% news_scraping_fun(in_url_list[j])
  #返回结果
  return(result)  #不用也可以
}
```


```{r}
microbenchmark(
  scraping_do = all_scraping_fun_do(68),
  scraping_par = all_scraping_fun(68),
  times = 1,
  unit = "s"
)
```

